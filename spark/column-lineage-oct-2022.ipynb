{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8867600e",
   "metadata": {},
   "source": [
    "# Column Lineage Demo - October 2022\n",
    "\n",
    "In this document we provide a demo of current implementation of Column Level Lineage within Marquez and Openlinege \n",
    "\n",
    "## Envirnonment setup \n",
    "\n",
    " * I've tested this demo on `Mac 12.6` with `Docker Desktop 4.12.0`. \n",
    " * I've setup Spark Juper environment like described here [here](https://openlineage.io/docs/integrations/spark/quickstart_local)\n",
    " * I've build and run Marquez locally because I wanted to work on master branch (requires Java 17):\n",
    " ```\n",
    " ./gradlew clean build\n",
    " ./gradlew :api:runShadow\n",
    " ```\n",
    "Once an October version of Marquez is released, testing this with Marquez docker image may be an easier option.  \n",
    " \n",
    "Let's check first if Marquez instance is properly running under a defined address. Returned status code should be 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b43f2f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marquez is OK.\n"
     ]
    }
   ],
   "source": [
    "import json,requests\n",
    "marquez_url = \"http://host.docker.internal:8080\" ## this may depend on your local setup\n",
    "if (requests.get(\"{}/api/v1/namespaces\".format(marquez_url)).status_code == 200):\n",
    "    print(\"Marquez is OK.\")\n",
    "else:\n",
    "    print(\"Cannot connect to Marquez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c5b2f",
   "metadata": {},
   "source": [
    "Let's create the warehouse directory first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e8b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir /home/jovyan/notebooks/spark-warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe60ddb",
   "metadata": {},
   "source": [
    "If Marquez connection is OK, we can start Spark context with OpenLineage pointed to Marquez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "914e5905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder.master('local')\n",
    "         .appName('sample_spark')\n",
    "         .config('spark.extraListeners', 'io.openlineage.spark.agent.OpenLineageSparkListener')\n",
    "         .config('spark.jars.packages', 'io.openlineage:openlineage-spark:0.15.1')\n",
    "         .config('spark.openlineage.url', '{}/api/v1/namespaces/column-lineage/'.format(marquez_url))\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc086ab",
   "metadata": {},
   "source": [
    "Let's clear docker's existing warehouse, in case the previous data exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a011bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf /home/jovyan/notebooks/spark-warehouse/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc940c6",
   "metadata": {},
   "source": [
    "## Run example Spark Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e7f340",
   "metadata": {},
   "source": [
    "Let's create now four datasets: `dataset_a`, `dataset_b`, `dataset_c`, `dataset_d`:\n",
    " * `dataset_a` has to columns `col_1` and `col_2` filled with some data,\n",
    " * `dataset_b` has one column `col_3` and is created from `dataset_a`,\n",
    " * `dataset_c` with `col_4` and `dataset_d` with `col_5` are created from `dataset_b`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f401c1e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/11 15:46:09 WARN HadoopFSUtils: The directory file:/home/jovyan/notebooks/spark-warehouse/dataset_c was not found. Was it deleted very recently?\n",
      "22/10/11 15:46:10 WARN HadoopFSUtils: The directory file:/home/jovyan/notebooks/spark-warehouse/dataset_d was not found. Was it deleted very recently?\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame([\n",
    "    {'col_1': 1, 'col_2': 2},\n",
    "    {'col_1': 3, 'col_2': 4}\n",
    "]).write.mode(\"overwrite\").saveAsTable('dataset_a')\n",
    "spark.sql(\"SELECT col_1 + col_2 AS col_3 FROM dataset_a\").write.mode(\"overwrite\").saveAsTable('dataset_b')\n",
    "spark.sql(\"SELECT col_3 AS col_4 FROM dataset_b\").write.mode(\"overwrite\").saveAsTable('dataset_c')\n",
    "spark.sql(\"SELECT col_3 AS col_5 FROM dataset_b\").write.mode(\"overwrite\").saveAsTable('dataset_d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4143ccb8",
   "metadata": {},
   "source": [
    "This should result in following column lineage graph:\n",
    " * `col_3` is created out of `col_1` and `col_2`,\n",
    " * `col_4` and `col_5` depend on `col_3`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94092c13",
   "metadata": {},
   "source": [
    "## Marquez API "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40563cf7",
   "metadata": {},
   "source": [
    "### Get dataset resource with column lineage included\n",
    "\n",
    "First we may list some example datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f9abcf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"namespace\": \"file\",\n",
      "  \"name\": \"/home/jovyan/notebooks/spark-warehouse/dataset_a\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "datasets = requests.get(\"{}/api/v1/namespaces/file/datasets\".format(marquez_url)).json()\n",
    "print(json.dumps(datasets[\"datasets\"][0][\"id\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b64488",
   "metadata": {},
   "source": [
    "Let's try now to fetch a specific dataset:\n",
    " * **namespace**: `file`,\n",
    " * **name**: `/home/jovyan/notebooks/spark-warehouse/dataset_c`\n",
    " \n",
    "We need to encode dataset name to be able to pass it through URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e2c674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "encoded_name = urllib.parse.quote_plus(\"/home/jovyan/notebooks/spark-warehouse/dataset_c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dbcf02",
   "metadata": {},
   "source": [
    "`dataset_c` was created from a single column `col_3` in `dataset_b`, so its column lineage section should only contain a single field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "486a7e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"name\": \"col_4\",\n",
      "    \"inputFields\": [\n",
      "      {\n",
      "        \"namespace\": \"file\",\n",
      "        \"dataset\": \"/home/jovyan/notebooks/spark-warehouse/dataset_b\",\n",
      "        \"field\": \"col_3\"\n",
      "      }\n",
      "    ],\n",
      "    \"transformationDescription\": null,\n",
      "    \"transformationType\": null\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "dataset = requests.get(\"{}/api/v1/namespaces/file/datasets/{}\".format(marquez_url, encoded_name)).json()\n",
    "print(json.dumps(dataset[\"columnLineage\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f24b59d",
   "metadata": {},
   "source": [
    "Fields `transformationDescription` and `transformationType` are available in the OpenLineage standard specification but not implemented in Spark integration (which is the only one).\n",
    "\n",
    "Column lineage within dataset resource does not return a whole column lineage graph. This is a desired behaviour as a separate column lineage endpoint is intended to fetch furter dependencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056d19",
   "metadata": {},
   "source": [
    "### Get column lineage graph\n",
    "\n",
    "Column-lineage endpoint returns a lineage graph by specified starting point (`nodeId`) and depth which is 20 by default. Starting point can be a dataset field or a dataset where dataset is equivalent to adding all dataset fields as starting points:\n",
    "\n",
    "* `nodeId=dataset:some-namespace:some-dataset`\n",
    "* `nodeId=datasetField:some-namespace:some-dataset:some-Field`\n",
    "\n",
    "We can distinguish **upstream** and **downstream** lineages. The endpoint by default returns only **upstream** lineage. In other words, it returns all columns that were used to produce a requsted fields while omitting fields that were produced by requested fields. \n",
    "\n",
    "Some datasets and fields can be used by hundreds or thousands of jobs, so we decided to avoid sending this information by default. This can be achieved with an extra request param `withDownstream` presented later.\n",
    "\n",
    "Let's check now the column lineage graph of `col_4` in `dataset_c`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "600279a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"graph\": [\n",
      "    {\n",
      "      \"id\": \"datasetField:file:/home/jovyan/notebooks/spark-warehouse/dataset_a:col_1\",\n",
      "      \"type\": \"DATASET_FIELD\",\n",
      "      \"data\": null,\n",
      "      \"inEdges\": [],\n",
      "      \"outEdges\": [\n",
      "        {\n",
      "          \"origin\": \"datasetField:file:/home/jovyan/notebooks/spark-warehouse/dataset_a:col_1\",\n",
      "          \"destination\": \"datasetField:file:/home/jovyan/notebooks/spark-warehouse/dataset_b:col_3\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"datasetField:file:/home/jovyan/notebooks/spark-warehouse/dataset_a:col_2\",\n",
      "      \"type\": \"DATASET_FIELD\",\n",
      "      \"data\": null,\n",
      "      \"inEdges\": [],\n",
      "      \"outEdges\": [\n",
      "        {\n",
      "          \"origin\": \"datasetField:file:/home/jovyan/notebooks/spark-warehouse/dataset_a:col_2\",\n",
      "          \"destination\": \"datasetField:file:/home/jovyan/notebooks/spark-warehouse/dataset_b:col_3\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"datasetField:file:/home/jovyan/notebooks/spark-warehouse/dataset_b:col_3\",\n",
      "      \"type\": \"DATASET_FIELD\",\n",
      "      \"data\": null,\n",
      "      \"inEdges\": [\n",
      "        {\n",
      "          \"origin\": \"datasetField:file:/home/jovyan/notebooks/spark-warehouse/dataset_b:col_3\",\n",
      "          \"destination\": \"datasetField:file:/home/jovyan/notebooks/spark-warehouse/dataset_a:col_1\"\n",
      "        },\n",
      "        {\n",
      "          \"origin\": \"datasetField:file:/home/jovyan/notebooks/spark-warehouse/dataset_b:col_3\",\n",
      "          \"destination\": \"datasetField:file:/home/jovyan/notebooks/spark-warehouse/dataset_a:col_2\"\n",
      "        }\n",
      "      ],\n",
      "      \"outEdges\": [\n",
      "        {\n",
      "          \"origin\": \"datasetField:file:/home/jovyan/notebooks/spark-warehouse/dataset_b:col_3\",\n",
      "          \"destination\": \"datasetField:file:/home/jovyan/notebooks/spark-warehouse/dataset_c:col_4\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"datasetField:file:/home/jovyan/notebooks/spark-warehouse/dataset_c:col_4\",\n",
      "      \"type\": \"DATASET_FIELD\",\n",
      "      \"data\": {\n",
      "        \"type\": \"DATASET_FIELD\",\n",
      "        \"namespace\": \"file\",\n",
      "        \"dataset\": \"/home/jovyan/notebooks/spark-warehouse/dataset_c\",\n",
      "        \"field\": \"col_4\",\n",
      "        \"fieldType\": \"long\",\n",
      "        \"transformationDescription\": null,\n",
      "        \"transformationType\": null,\n",
      "        \"inputFields\": [\n",
      "          {\n",
      "            \"namespace\": \"file\",\n",
      "            \"dataset\": \"/home/jovyan/notebooks/spark-warehouse/dataset_b\",\n",
      "            \"field\": \"col_3\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"inEdges\": [\n",
      "        {\n",
      "          \"origin\": \"datasetField:file:/home/jovyan/notebooks/spark-warehouse/dataset_c:col_4\",\n",
      "          \"destination\": \"datasetField:file:/home/jovyan/notebooks/spark-warehouse/dataset_b:col_3\"\n",
      "        }\n",
      "      ],\n",
      "      \"outEdges\": []\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(requests.get(\n",
    "    \"{}/api/v1/column-lineage?nodeId=datasetField:file:{}:col_4\".format(marquez_url, encoded_name)\n",
    ").json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f441307",
   "metadata": {},
   "source": [
    "It contains `dataset_b:col_3`, `dataset_a:col_1` and `dataset_a:col_2` as nodes, while `dataset_d` is not not returned as it is unrelated to column lingea of the requested node. \n",
    "\n",
    "Response is returned in a form of `Lineage` graph, the same as existing `lineage` endpoint in Marquez. It contains dataset fields as graph nodesm, each with data section containing node's information and edges attached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73dfcbb",
   "metadata": {},
   "source": [
    "### Downstream column lineage \n",
    "\n",
    "Lineage endpoint returns by default only upstream lineage. \n",
    "In order to fetch downstream, an extra parameter `withDownstream=true` has to be added. \n",
    "\n",
    "We will test it on `dataset_b` and verify that only upstream columns `dataset_a:col_1`, `dataset_a:col_2` are returned, but also `dataset_c:col_4` and `dataset_d:col_5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f94df91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_c_encoded_name = urllib.parse.quote_plus(\"/home/jovyan/notebooks/spark-warehouse/dataset_c\");\n",
    "\n",
    "# print(json.dumps(requests.get(\n",
    "#     \"{}/api/v1/column-lineage?nodeId=datasetField:file:{}:col_4&withDownstream=true\"\n",
    "#     .format(marquez_url, dataset_c_encoded_name)\n",
    "# ).json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571edcc8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Alert:</b> Downstream lineage has a bug. The same edge is being traversed up and down resulting in multiple input fields for `dataset_b:col_3`. Still work in progress.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2787e8f",
   "metadata": {},
   "source": [
    "This ends the demo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
