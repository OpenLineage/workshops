{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f05f4980-726c-4e98-aa5b-6cb347e313fb",
   "metadata": {},
   "source": [
    "# Iceberg Summit 2025 demo\n",
    "\n",
    "Let's verify now if the Marquez is app and running "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87da5835-5414-4ec5-b730-9ba7125448af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marquez is OK.\n"
     ]
    }
   ],
   "source": [
    "import json,requests\n",
    "marquez_url = \"http://marquez:9000\" ## this may depend on your local setup\n",
    "if (requests.get(\"{}/api/v1/namespaces\".format(marquez_url)).status_code == 200):\n",
    "    print(\"Marquez is OK.\")\n",
    "else:\n",
    "    print(\"Cannot connect to Marquez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d78e0d-790a-4d8b-8569-7c8a124da2d9",
   "metadata": {},
   "source": [
    "You can check it in your browser as well -> http://localhost:3000\n",
    "\n",
    "Let's now start Spark session with Openlineage and Iceberg enabled: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a0f1b38-09f7-4584-b632-5069d16c8fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder.master('local')\n",
    "     .appName('sample_spark')\n",
    "     .config('spark.jars.packages', 'io.openlineage:openlineage-spark_2.12:1.30.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.8.1')\n",
    "     .config('spark.extraListeners', 'io.openlineage.spark.agent.OpenLineageSparkListener')\n",
    "     .config('spark.openlineage.transport.type', 'http')\n",
    "     .config('spark.openlineage.transport.url', 'http://marquez:9000/api/v1')\n",
    "     .config('spark.openlineage.job.owners.team', 'capybaras')\n",
    "     .config('spark.openlineage.job.owners.email', 'capybaras@myorg.com')\n",
    "     .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "     .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\")\n",
    "     .config(\"spark.sql.catalog.spark_catalog.warehouse\", \"/tmp/iceberg\")\n",
    "     .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "     .getOrCreate())\n",
    "spark.sparkContext.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027a38af-b56b-451e-b7f4-ac20bc8d3436",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "### Downlaod tripdata\n",
    "\n",
    "We download NYC taxi data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf81a0f2-892c-470d-a1ac-e8e899b8987b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading 01\n",
      "downloading 02\n",
      "downloading 03\n",
      "downloading 04\n",
      "downloading 05\n",
      "downloading 06\n",
      "downloading 07\n",
      "downloading 08\n",
      "downloading 09\n",
      "downloading 10\n",
      "deliberately skipping November\n",
      "downloading 12\n"
     ]
    }
   ],
   "source": [
    "# data from https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "import os\n",
    "os.system(f\"rm -rf taxi_parquet\")\n",
    "for i in range(1, 13):\n",
    "    if (i == 11):\n",
    "        print(\"deliberately skipping November\")\n",
    "        continue\n",
    "    print(\"downloading\", str(i).zfill(2))\n",
    "    os.system(f\"wget -c https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-{str(i).zfill(2)}.parquet -P taxi_parquet \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ee343c-4113-4707-a016-10198615785d",
   "metadata": {},
   "source": [
    "### Write data as iceberg table with ride_id UUID column added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d19f47e-afd5-4829-8439-9e6656b2ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from pyspark.sql.functions import udf, month\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "uuidUdf= udf(lambda : str(uuid.uuid4()),StringType())\n",
    "\n",
    "spark \\\n",
    "  .read \\\n",
    "  .parquet(\"taxi_parquet\") \\\n",
    "  .withColumn(\"ride_id\", uuidUdf()) \\\n",
    "  .withColumn(\"month\", month(\"tpep_dropoff_datetime\")) \\\n",
    "  .write \\\n",
    "  .format(\"iceberg\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .partitionBy(\"month\") \\\n",
    "  .saveAsTable(\"yellow_tripdata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427bafe3-e11f-467a-8b3c-e7d41dacf374",
   "metadata": {},
   "source": [
    "## Create extra table with users' comments for 1% of the rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "929b4956-6771-4708-8182-5c5d737b0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "spark \\\n",
    "  .read \\\n",
    "  .table(\"yellow_tripdata\") \\\n",
    "  .sample(fraction=0.01) \\\n",
    "  .select(col(\"ride_id\"), col(\"tpep_pickup_datetime\").alias(\"datetime\")) \\\n",
    "  .withColumn(\"comment\", lit(\"This ride was cool\")) \\\n",
    "  .write \\\n",
    "  .format(\"iceberg\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .saveAsTable(\"comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7845efc4-b61e-4da6-99e2-db1fcf35f2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------------------+\n",
      "|             ride_id|           datetime|           comment|\n",
      "+--------------------+-------------------+------------------+\n",
      "|29d49cdb-b7b5-4dc...|2024-10-01 00:09:29|This ride was cool|\n",
      "|55dcb827-0fe9-472...|2024-10-01 00:09:16|This ride was cool|\n",
      "+--------------------+-------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"comments\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c775a573-0064-4001-b791-bad646e25e30",
   "metadata": {},
   "source": [
    "## Common pitfall\n",
    "\n",
    "Get one-month comments joined with trip_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "780457e9-7f60-4f6e-ac1f-4916d40d1451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month\n",
    "\n",
    "comments = spark.read.table(\"comments\")\n",
    "tripdata = spark.read.table(\"yellow_tripdata\")\n",
    "\n",
    "\n",
    "tripdata \\\n",
    "  .join(comments, tripdata[\"ride_id\"] == comments[\"ride_id\"]) \\\n",
    "  .filter(month(comments[\"datetime\"]) == 12) \\\n",
    "  .select(comments[\"comment\"], tripdata[\"trip_distance\"]) \\\n",
    "  .write \\\n",
    "  .format(\"iceberg\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .saveAsTable(\"comment_distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0646fb-04ba-4ad6-8d99-728b34e374c4",
   "metadata": {},
   "source": [
    "But there is something wrong with the query below as we don't filter month on tripdata datataset. \n",
    "\n",
    "The query should return proper results but this is a clear performance BUG. \n",
    "\n",
    "One can see the graph here:\n",
    "http://localhost:3000/lineage/job/default/sample_spark.atomic_replace_table_as_select.default_comment_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159969be-bd78-42cc-ae14-520ddf3ee06f",
   "metadata": {},
   "source": [
    "## Fixed query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18d73df9-a8ec-4e6a-8214-f6adeaab7843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month\n",
    "\n",
    "comments = spark.read.table(\"comments\")\n",
    "tripdata = spark.read.table(\"yellow_tripdata\")\n",
    "\n",
    "\n",
    "tripdata \\\n",
    "  .filter(\"month=12\") \\\n",
    "  .join(comments, tripdata[\"ride_id\"] == comments[\"ride_id\"]) \\\n",
    "  .filter(month(comments[\"datetime\"]) == 12) \\\n",
    "  .select(comments[\"comment\"], tripdata[\"trip_distance\"]) \\\n",
    "  .write \\\n",
    "  .format(\"iceberg\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .saveAsTable(\"comment_distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2519b2f0-0164-45fa-93da-297887ef29ad",
   "metadata": {},
   "source": [
    "## Identify problematic jobs and contact teams \n",
    "\n",
    "List all jobs that that read more than X bytes and with skipped files in metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c009d7-4b46-46cb-a8da-714139824ad3",
   "metadata": {},
   "source": [
    "You can see the lineage graph in [Marquez](http://localhost:3000/lineage/job/default/sample_spark.atomic_replace_table_as_select.default_comment_distance_december):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e32aadfe-4f17-44dd-8ea9-91ecbf823be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "response = requests.get(\"{}/api/v1/events/lineage?limit=100\".format(marquez_url))\n",
    "events = json.loads(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa0a17-a3f5-4a8a-a8eb-e755350e5773",
   "metadata": {},
   "source": [
    "List all jobs that read more than 1 gigabyte of data without skippedFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ac2f424-8b3b-42d7-9c4c-db7cf694e9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Job :sample_spark.append_data.spark_catalog_default_comment_distance \n",
      "reading '/tmp/iceberg/default/yellow_tripdata' \n",
      "owner: [{'name': 'capybaras', 'type': 'team'}, {'name': 'capybaras@myorg.com', 'type': 'email'}] \n",
      "has read: 1.27 gigabyte\n",
      "\n",
      "\n",
      "Job :sample_spark.append_data.spark_catalog_default_comments \n",
      "reading '/tmp/iceberg/default/yellow_tripdata' \n",
      "owner: [{'name': 'capybaras', 'type': 'team'}, {'name': 'capybaras@myorg.com', 'type': 'email'}] \n",
      "has read: 1.27 gigabyte\n"
     ]
    }
   ],
   "source": [
    "for event in events['events']:\n",
    "    jobName = event['job']['name']\n",
    "    owner = event['job']['facets'].get('ownership')\n",
    "    for dataset in event['inputs']:\n",
    "        if dataset.get('inputFacets') == None or owner == None:\n",
    "            continue\n",
    "        scan = dataset['inputFacets'].get('icebergScanReport')\n",
    "        if scan != None:\n",
    "            skippedFiles = scan['scanMetrics']['skippedDataFiles']\n",
    "            bytesRead = scan['scanMetrics']['totalFileSizeInBytes']\n",
    "            # more than 1Gigabyte read and no skippedFiles\n",
    "            if skippedFiles == 0 and bytesRead > 1048576000:\n",
    "                print(\n",
    "                    f\"\\n\\nJob :{jobName}\", \n",
    "                    f\"\\nreading '{dataset['name']}'\",\n",
    "                    f\"\\nowner: {owner.get('owners')}\", \n",
    "                    f\"\\nhas read: {bytesRead/(1<<30):.2f}\", \"gigabyte\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc6c7f5-f51c-437f-a0de-5f4a44400d74",
   "metadata": {},
   "source": [
    "## Missing November data\n",
    "\n",
    "Let's fill comment_distance table for November."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf31c3d9-452b-499f-886d-50010940d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tripdata \\\n",
    "  .filter(\"month=11\") \\\n",
    "  .join(comments, tripdata[\"ride_id\"] == comments[\"ride_id\"]) \\\n",
    "  .filter(month(comments[\"datetime\"]) == 11) \\\n",
    "  .select(comments[\"comment\"], tripdata[\"trip_distance\"]) \\\n",
    "  .write \\\n",
    "  .format(\"iceberg\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .saveAsTable(\"comment_distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1c3672-a128-48af-a616-6ea12c6b7f52",
   "metadata": {},
   "source": [
    "This should not append any rows. Let's compare now the CommitReports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75516e32-c41e-4027-8c9c-780530f8dde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"{}/api/v1/events/lineage?limit=50\".format(marquez_url))\n",
    "events = json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a2a102e-9b57-45fd-84e5-61ba0db3ead3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Job :sample_spark.append_data.spark_catalog_default_comment_distance \n",
      "writing '/tmp/iceberg/default/comment_distance' \n",
      "runId: 0195a84b-0ef5-762d-9b02-3175e3a2a9c7 \n",
      "has written: 1 records\n",
      "\n",
      "\n",
      "Job :sample_spark.atomic_replace_table_as_select.default_comment_distance \n",
      "writing '/tmp/iceberg/default/comment_distance' \n",
      "runId: 0195a84a-4012-73cd-bab4-c54db77dbc9d \n",
      "has written: 36904 records\n",
      "\n",
      "\n",
      "Job :sample_spark.append_data.spark_catalog_default_comment_distance \n",
      "writing '/tmp/iceberg/default/comment_distance' \n",
      "runId: 0195a84a-406f-7685-8ec1-d4ecd0701e4c \n",
      "has written: 36907 records\n",
      "\n",
      "\n",
      "Job :sample_spark.atomic_replace_table_as_select.default_comment_distance \n",
      "writing '/tmp/iceberg/default/comment_distance' \n",
      "runId: 0195a84a-036c-704c-92e8-3ab7e67e9794 \n",
      "has written: 36907 records\n",
      "\n",
      "\n",
      "Job :sample_spark.append_data.spark_catalog_default_comment_distance \n",
      "writing '/tmp/iceberg/default/comment_distance' \n",
      "runId: 0195a84a-03f5-758e-a074-0130bdeb02d4 \n",
      "has written: 36907 records\n"
     ]
    }
   ],
   "source": [
    "for event in events['events']:\n",
    "    runId = event['run'].get('runId')\n",
    "    jobName = event['job']['name']\n",
    "    for dataset in event['outputs']:\n",
    "        if not dataset.get('name').endswith('comment_distance'):\n",
    "            continue\n",
    "        if dataset.get('outputFacets') == None:\n",
    "            continue\n",
    "        commit = dataset['outputFacets'].get('icebergCommitReport')\n",
    "        if commit != None:\n",
    "            print(\n",
    "                    f\"\\n\\nJob :{jobName}\", \n",
    "                    f\"\\nwriting '{dataset['name']}'\",\n",
    "                    f\"\\nrunId: {runId}\", \n",
    "                    f\"\\nhas written: {commit['commitMetrics'].get('addedRecords')}\", \"records\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b44534-c048-4ee3-bae9-da0b673f9e33",
   "metadata": {},
   "source": [
    "This is interesting. Why a certain run did not write any record? Lets verify the inputs of jobs writing to `comment_distance`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "096b6fa5-8719-401e-8dd6-cb1a88b3c3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Job :sample_spark.append_data.spark_catalog_default_comment_distance run at 2025-03-18T08:07:26.385Z \n",
      "Iutput dataset :/tmp/iceberg/default/yellow_tripdata \n",
      "rows read '1994'\n",
      "\n",
      "\n",
      "Job :sample_spark.append_data.spark_catalog_default_comment_distance run at 2025-03-18T08:07:26.385Z \n",
      "Iutput dataset :/tmp/iceberg/default/comments \n",
      "rows read '376458'\n",
      "\n",
      "\n",
      "Job :sample_spark.append_data.spark_catalog_default_comment_distance run at 2025-03-18T08:06:34.763Z \n",
      "Iutput dataset :/tmp/iceberg/default/yellow_tripdata \n",
      "rows read '3667755'\n",
      "\n",
      "\n",
      "Job :sample_spark.append_data.spark_catalog_default_comment_distance run at 2025-03-18T08:06:34.763Z \n",
      "Iutput dataset :/tmp/iceberg/default/comments \n",
      "rows read '376458'\n",
      "\n",
      "\n",
      "Job :sample_spark.append_data.spark_catalog_default_comment_distance run at 2025-03-18T08:06:32.892Z \n",
      "Iutput dataset :/tmp/iceberg/default/yellow_tripdata \n",
      "rows read '37523351'\n",
      "\n",
      "\n",
      "Job :sample_spark.append_data.spark_catalog_default_comment_distance run at 2025-03-18T08:06:32.892Z \n",
      "Iutput dataset :/tmp/iceberg/default/comments \n",
      "rows read '376458'\n"
     ]
    }
   ],
   "source": [
    "for event in events['events']:\n",
    "    if len(event['outputs']) == 0:\n",
    "        continue\n",
    "    if not event['outputs'][0]['name'].endswith('comment_distance'):\n",
    "        continue\n",
    "    jobName = event['job']['name']\n",
    "    for dataset in event['inputs']:\n",
    "        if dataset.get('inputFacets') == None or dataset.get('inputFacets').get('icebergScanReport') == None:\n",
    "            continue\n",
    "        scan = dataset['inputFacets'].get('icebergScanReport')\n",
    "        if scan != None:\n",
    "              print(\n",
    "                f\"\\n\\nJob :{jobName} run at {event['eventTime']}\", \n",
    "                f\"\\nIutput dataset :{dataset['name']}\", \n",
    "                f\"\\nrows read '{dataset['inputFacets'].get('inputStatistics').get('rowCount')}'\"\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d86872e-0e7e-4acf-8967-21593abaaf9e",
   "metadata": {},
   "source": [
    "This is something that cannot be done purely with Iceberg Metrics on its own. The value of the metric comes from connecting input metrics with output metrics, while they belong to the same job."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
