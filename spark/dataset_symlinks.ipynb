{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c2c062",
   "metadata": {},
   "source": [
    "# Dataset symlink demo\n",
    "\n",
    "Sometimes it happens that a dataset is identified by different name depending on a context. This may happen for Spark tables which can be identified by their physical location or identifier made of catalog and table name (logical identifier). This inconsistency can lead to broken lineage: imagine one job writing to a physical location and the other reading from a catalog and table name. With no extra information provided, the backend is not able to produce a consistent lineage of that. \n",
    "\n",
    "Dataset symlink feature is an extra facet within dataset saying: *Hey, these are the alternate names of the dataset*.\n",
    "\n",
    "The information is stored on the Marquez side and, what even more important, it is used to identify datasets by all its possible names. If a dataset primary name is a physical location, then we will be able to retrieve it by its logical name (catalog and table name) if such were provided within symlink facet. All the names will be used to create edges of the lineage graph. \n",
    "\n",
    "This demo presents the above in action. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd961fa9",
   "metadata": {},
   "source": [
    "Let us first verify if Marquez is up and running "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e80da235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marquez is OK.\n"
     ]
    }
   ],
   "source": [
    "import json,requests\n",
    "marquez_url = \"http://marquez:5000\" ## this may depend on your local setup\n",
    "if (requests.get(\"{}/api/v1/namespaces\".format(marquez_url)).status_code == 200):\n",
    "    print(\"Marquez is OK.\")\n",
    "else:\n",
    "    print(\"Cannot connect to Marquez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef2d003",
   "metadata": {},
   "source": [
    "Next we spin a Spark cluster with an Openlineage connector enabled.\n",
    "We enable Hive support for the Spark session and specify fake Hive metastore \"http://metastore\". Trick is done to emulate a behaviour of having two possible identifiers for a dataset:\n",
    " * its physical location on disk,\n",
    " * its logical location identified by a catalog and table name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cdacc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "spark = (SparkSession.builder.master('local')\n",
    "         .appName('sample_spark')\n",
    "         .config('spark.extraListeners', 'io.openlineage.spark.agent.OpenLineageSparkListener')\n",
    "         .config('spark.jars.packages', 'io.openlineage:openlineage-spark:0.15.1')\n",
    "         .config('spark.openlineage.url', '{}/api/v1/namespaces/dataset-symlinks/'.format(marquez_url))\n",
    "         .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "         .config(\"spark.sql.hive.metastore.uris\", \"http://metastore\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6c75b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/25 12:57:53 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "22/10/25 12:57:53 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "22/10/25 12:57:57 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "22/10/25 12:57:57 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.18.0.4\n",
      "22/10/25 12:58:00 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "22/10/25 12:58:00 WARN LogicalPlanSerializer: Can't register jackson scala module for serializing LogicalPlan\n",
      "22/10/25 12:58:01 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "22/10/25 12:58:01 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "22/10/25 12:58:01 WARN DropTableCommandVisitor: Unable to find table by identifier `default`.`some_table` - Table or view 'some_table' not found in database 'default'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/25 12:58:02 WARN DropTableCommandVisitor: Unable to find table by identifier `default`.`some_table` - Table or view 'some_table' not found in database 'default'\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS some_table;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c4169ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/25 12:59:46 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "22/10/25 12:59:47 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "22/10/25 12:59:47 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "22/10/25 12:59:47 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "22/10/25 12:59:47 WARN HiveMetaStore: Location: file:/home/jovyan/notebooks/spark-warehouse/some_table specified for non-external table:some_table\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS some_table (key INT, value STRING) USING hive;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243dae1b",
   "metadata": {},
   "source": [
    "Let's see the latest event sent to Marquez and its output datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96af5172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"namespace\": \"dataset-symlinks\",\n",
      "  \"name\": \"sample_spark.execute_create_table_command\",\n",
      "  \"facets\": {\n",
      "    \"documentation\": null,\n",
      "    \"sourceCodeLocation\": null,\n",
      "    \"sql\": null\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "event = requests.get(\"{}/api/v1/events/lineage?limit=1\".format(marquez_url)).json()['events'][0]\n",
    "\n",
    "print(json.dumps(event['job'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39a7341e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primiary dataset name is \n",
      "\t namespace:'file', \n",
      "\t name:'/home/jovyan/notebooks/spark-warehouse/some_table'\n",
      "\n",
      "Symlinks facet:\n",
      "{\n",
      "  \"_producer\": \"https://github.com/OpenLineage/OpenLineage/tree/0.15.1/integration/spark\",\n",
      "  \"_schemaURL\": \"https://openlineage.io/spec/facets/1-0-0/SymlinksDatasetFacet.json#/$defs/SymlinksDatasetFacet\",\n",
      "  \"identifiers\": [\n",
      "    {\n",
      "      \"namespace\": \"hive://metastore\",\n",
      "      \"name\": \"default.some_table\",\n",
      "      \"type\": \"TABLE\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# dataset name \n",
    "print(\"Primiary dataset name is \\n\\t namespace:'{}', \\n\\t name:'{}'\".format(\n",
    "    event['outputs'][0]['namespace'], \n",
    "    event['outputs'][0]['name']\n",
    "))\n",
    "print(\"\\nSymlinks facet:\")\n",
    "# and symlink name\n",
    "print(json.dumps(event['outputs'][0]['facets']['symlinks'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2675f3c",
   "metadata": {},
   "source": [
    "## Get dataset by different names\n",
    "\n",
    "Let's encode dataset name (`/home/jovyan/notebooks/spark-warehouse/some_table`), retrieve dataset by namespace and name and print its `id`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c72289b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"namespace\": \"file\",\n",
      "  \"name\": \"/home/jovyan/notebooks/spark-warehouse/some_table\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "dataset = requests.get(\n",
    "    \"{}/api/v1/namespaces/file/datasets/{}\".format(\n",
    "        marquez_url, \n",
    "        urllib.parse.quote_plus(event['outputs'][0]['name'])\n",
    "    )\n",
    ").json()\n",
    "print(json.dumps(dataset['id'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bed3f34",
   "metadata": {},
   "source": [
    "Let's now try to retrieve a dataset by a symlink name. We expect to get the same dataset although requesting by a different name: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9787a8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"namespace\": \"file\",\n",
      "  \"name\": \"/home/jovyan/notebooks/spark-warehouse/some_table\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataset = requests.get(\n",
    "    \"{}/api/v1/namespaces/{}/datasets/{}\".format(\n",
    "        marquez_url, \n",
    "        urllib.parse.quote_plus(\"hive://metastore\"),\n",
    "        \"default.some_table\"\n",
    "    )\n",
    ").json()\n",
    "print(json.dumps(dataset['id'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4618c7",
   "metadata": {},
   "source": [
    "## Create lineage graph with symlink edges\n",
    "\n",
    "Assume there is some other job that reads Hive table by its catalog location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "313e2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_job_event = {\n",
    "    'eventType': 'COMPLETE',\n",
    "    'eventTime': '2022-10-25T11:35:31.341Z',\n",
    "    'job': {\n",
    "        'namespace': 'another-job-namespace',\n",
    "        'name': 'another-job',\n",
    "        'facets': {'documentation': None, 'sourceCodeLocation': None, 'sql': None}\n",
    "    },\n",
    "    'run': {\n",
    "        'runId': 'ae8b3ab7-254b-4d81-9c0a-c152f6sdac90'\n",
    "    },\n",
    "    'inputs': [\n",
    "        {\n",
    "            'namespace': 'hive://metastore', # read a dataset by its logical name\n",
    "            'name': 'default.some_table'\n",
    "        }\n",
    "    ],\n",
    "    'outputs': [\n",
    "         {\n",
    "            'namespace': 'another-namespace', # write to some other dataset\n",
    "            'name': 'another-table'\n",
    "        }\n",
    "    ],\n",
    "    'producer': 'https://github.com/OpenLineage/OpenLineage/tree/0.15.1/integration/spark',\n",
    "    'schemaURL': 'https://openlineage.io/spec/1-0-3/OpenLineage.json#/$defs/RunEvent'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea7e6a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [201]>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.post(\n",
    "    \"{}/api/v1/lineage/\".format(marquez_url), \n",
    "    data=json.dumps(other_job_event), \n",
    "    headers={\"Content-Type\":\"application/json\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d0dad",
   "metadata": {},
   "source": [
    "Let's fetch lineage graph for `/home/jovyan/notebooks/spark-warehouse/some_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e005adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = requests.get(\n",
    "    \"{}/api/v1/lineage?nodeId=dataset:file:{}\".format(\n",
    "        marquez_url, \n",
    "        urllib.parse.quote_plus(\"/home/jovyan/notebooks/spark-warehouse/some_table\")\n",
    "    )\n",
    ").json()['graph']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956c70a7",
   "metadata": {},
   "source": [
    "and print nodes corresponding to dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3aedd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:another-namespace:another-table\n",
      "dataset:file:/home/jovyan/notebooks/spark-warehouse/some_table\n"
     ]
    }
   ],
   "source": [
    "for node in graph:\n",
    "    if node['type']=='DATASET':\n",
    "        print(node['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995d9616",
   "metadata": {},
   "source": [
    "Node representing a dataset `dataset:another-namespace:another-table` is on the list which means Marquez was able to build a linege dependency based on a symlink."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
